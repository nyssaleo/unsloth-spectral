"""
COMPREHENSIVE LONG-CONTEXT VALIDATION TEST SUITE
================================================

This script validates the Unsloth Spectral KV Cache implementation with extensive tests:
1. Unit Tests: Isolated component verification
2. Long Context Tests: Compression trigger verification (>512 tokens)
3. Needle-in-Haystack: Factual recall under compression
4. Memory Profiling: VRAM usage measurement
5. Quality Metrics: Generation comparison
6. Performance Benchmarks: Throughput measurement

Run in Google Colab with T4 GPU. Copy this entire file into a single code cell.

Author: Ankit Prajapati & Claude (Anthropic)
Date: January 2026
"""

# =============================================================================
# CELL 0: INSTALLATION & SETUP
# =============================================================================
print("=" * 70)
print("UNSLOTH SPECTRAL - COMPREHENSIVE VALIDATION SUITE")
print("=" * 70)

import subprocess
import sys
import os

def install_packages():
    """Install required packages with error handling."""
    packages = [
        ("triton", "triton"),
        ("unsloth", "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"),
        ("unsloth_spectral", "git+https://github.com/nyssaleo/unsloth-spectral.git"),
    ]
    
    for name, spec in packages:
        print(f"\nüì¶ Installing {name}...")
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pip", "install", "--quiet", spec],
                capture_output=True,
                text=True,
                timeout=300,
            )
            if result.returncode != 0:
                print(f"  ‚ö†Ô∏è Warning: pip install returned {result.returncode}")
                print(f"  stderr: {result.stderr[:500] if result.stderr else 'None'}")
            else:
                print(f"  ‚úÖ {name} installed")
        except subprocess.TimeoutExpired:
            print(f"  ‚ö†Ô∏è Timeout installing {name}")
        except Exception as e:
            print(f"  ‚ùå Error installing {name}: {e}")

# Run installation
install_packages()

# =============================================================================
# CELL 1: IMPORTS & ENVIRONMENT CHECK
# =============================================================================
print("\n" + "=" * 70)
print("ENVIRONMENT CHECK")
print("=" * 70)

import torch
import time
import math
import gc
from typing import Optional, Dict, List, Tuple

# Check PyTorch and CUDA
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# Check Triton
try:
    import triton
    print(f"Triton version: {triton.__version__}")
    TRITON_OK = True
except ImportError:
    print("Triton: Not available")
    TRITON_OK = False

# Import unsloth_spectral components
try:
    os.environ["UNSLOTH_SPECTRAL_QUIET"] = "1"  # Suppress banner
    
    from unsloth_spectral import (
        SpectralCache,
        SpectralBlock,
        patch_unsloth_attention,
        get_cache_stats,
        spectral_attention_forward,
    )
    
    # Try importing Triton kernel components
    try:
        from unsloth_spectral.kernels import (
            TRITON_AVAILABLE,
            TRITON_VERSION,
            spectral_attention_decode,
            spectral_score_forward,
            spectral_value_forward,
            TritonSpectralConfig,
        )
        print(f"Triton kernels: Available (v{TRITON_VERSION})")
    except ImportError as e:
        print(f"Triton kernels: Not available ({e})")
        TRITON_AVAILABLE = False
        TritonSpectralConfig = None
        spectral_attention_decode = None
        spectral_score_forward = None
        spectral_value_forward = None
    
    print("‚úÖ unsloth_spectral imported successfully")
    SPECTRAL_OK = True
    
except ImportError as e:
    print(f"‚ùå Failed to import unsloth_spectral: {e}")
    SPECTRAL_OK = False

# Import Unsloth
try:
    from unsloth import FastLanguageModel
    print("‚úÖ unsloth imported successfully")
    UNSLOTH_OK = True
except ImportError as e:
    print(f"‚ùå Failed to import unsloth: {e}")
    UNSLOTH_OK = False

print("\n" + "=" * 70)

# =============================================================================
# CELL 2: UNIT TESTS - SPECTRAL CACHE ISOLATION
# =============================================================================
def test_spectral_cache_unit():
    """Test SpectralCache in isolation (no model dependencies)."""
    print("\n" + "=" * 70)
    print("UNIT TEST 1: SpectralCache Isolation")
    print("=" * 70)
    
    if not SPECTRAL_OK:
        print("‚ùå Skipping: unsloth_spectral not available")
        return False
    
    # Configuration matching Mistral-7B
    H_kv = 8       # num_key_value_heads
    D = 128        # head_dim
    block_size = 512
    k_K = 16       # k_rank_keys
    k_V = 32       # k_rank_values
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16 if device == "cuda" else torch.float32
    
    print(f"Config: H_kv={H_kv}, D={D}, block_size={block_size}, k_K={k_K}, k_V={k_V}")
    print(f"Device: {device}, dtype: {dtype}")
    
    # Create cache
    cache = SpectralCache(
        num_heads=H_kv,
        head_dim=D,
        block_size=block_size,
        k_rank_keys=k_K,
        k_rank_values=k_V,
        device=device,
        dtype=dtype,
        debug_logging=False,
    )
    
    print(f"\n‚úÖ Cache created: {cache}")
    
    # Test 1: Append small batch (no compression)
    print("\n--- Test 1: Small append (no compression) ---")
    K1 = torch.randn(1, H_kv, 100, D, device=device, dtype=dtype)
    V1 = torch.randn(1, H_kv, 100, D, device=device, dtype=dtype)
    cache.append(K1, V1)
    
    print(f"After 100 tokens: total={cache.total_tokens}, cold_blocks={len(cache.cold_blocks)}")
    assert cache.total_tokens == 100, f"Expected 100, got {cache.total_tokens}"
    assert len(cache.cold_blocks) == 0, f"Expected 0 blocks, got {len(cache.cold_blocks)}"
    print("‚úÖ Small append passed")
    
    # Test 2: Append to trigger compression
    print("\n--- Test 2: Append to trigger compression (>512 tokens) ---")
    K2 = torch.randn(1, H_kv, 500, D, device=device, dtype=dtype)
    V2 = torch.randn(1, H_kv, 500, D, device=device, dtype=dtype)
    cache.append(K2, V2)
    
    print(f"After 600 tokens: total={cache.total_tokens}")
    print(f"  Cold blocks: {len(cache.cold_blocks)}")
    print(f"  Hot tokens: {cache.hot_K.shape[2] if cache.hot_K is not None else 0}")
    
    expected_cold = 1  # 512 tokens compressed
    expected_hot = 88  # 600 - 512 = 88
    assert len(cache.cold_blocks) == expected_cold, f"Expected {expected_cold} blocks, got {len(cache.cold_blocks)}"
    if cache.hot_K is not None:
        actual_hot = cache.hot_K.shape[2]
        assert actual_hot == expected_hot, f"Expected {expected_hot} hot tokens, got {actual_hot}"
    print("‚úÖ Compression triggered correctly")
    
    # Test 3: Verify spectral block structure
    print("\n--- Test 3: Spectral block structure ---")
    block = cache.cold_blocks[0]
    print(f"Block 0:")
    print(f"  coeffs_K: {block.coeffs_K.shape} (expected: [H_kv={H_kv}, block_size={block_size}, k_K={k_K}])")
    print(f"  basis_K: {block.basis_K.shape} (expected: [H_kv={H_kv}, k_K={k_K}, D={D}])")
    print(f"  coeffs_V: {block.coeffs_V.shape} (expected: [H_kv={H_kv}, block_size={block_size}, k_V={k_V}])")
    print(f"  basis_V: {block.basis_V.shape} (expected: [H_kv={H_kv}, k_V={k_V}, D={D}])")
    print(f"  dtype: coeffs_K={block.coeffs_K.dtype}, basis_K={block.basis_K.dtype}")
    
    assert block.coeffs_K.shape == (H_kv, block_size, k_K), f"coeffs_K shape mismatch"
    assert block.basis_K.shape == (H_kv, k_K, D), f"basis_K shape mismatch"
    assert block.coeffs_V.shape == (H_kv, block_size, k_V), f"coeffs_V shape mismatch"
    assert block.basis_V.shape == (H_kv, k_V, D), f"basis_V shape mismatch"
    print("‚úÖ Spectral block structure correct")
    
    # Test 4: Full reconstruction
    print("\n--- Test 4: Full K/V reconstruction ---")
    K_full, V_full = cache.get_kv()
    print(f"Reconstructed: K={K_full.shape}, V={V_full.shape}")
    
    expected_shape = (1, H_kv, 600, D)
    assert K_full.shape == expected_shape, f"K shape mismatch: {K_full.shape} vs {expected_shape}"
    assert V_full.shape == expected_shape, f"V shape mismatch: {V_full.shape} vs {expected_shape}"
    print("‚úÖ Full reconstruction shape correct")
    
    # Test 5: Memory statistics
    print("\n--- Test 5: Memory statistics ---")
    stats = cache.get_memory_stats()
    print(f"Memory stats:")
    print(f"  Original: {stats['original_bytes'] / 1024:.1f} KB")
    print(f"  Compressed: {stats['compressed_bytes'] / 1024:.1f} KB")
    print(f"  Compression ratio: {stats['compression_ratio']:.2f}x")
    print(f"  Blocks: {stats['num_blocks']}, Tokens: {stats['total_tokens']}")
    
    # With compression, ratio should be > 1
    if stats['num_blocks'] > 0:
        assert stats['compression_ratio'] > 1.0, f"Expected compression_ratio > 1, got {stats['compression_ratio']}"
        print("‚úÖ Compression active")
    else:
        print("‚ö†Ô∏è No compressed blocks (expected in short context)")
    
    # Test 6: Reset functionality
    print("\n--- Test 6: Cache reset ---")
    cache.reset()
    assert cache.total_tokens == 0, f"Expected 0 tokens after reset, got {cache.total_tokens}"
    assert len(cache.cold_blocks) == 0, f"Expected 0 blocks after reset"
    print("‚úÖ Cache reset works")
    
    print("\n" + "=" * 70)
    print("UNIT TEST 1: ‚úÖ ALL PASSED")
    print("=" * 70)
    return True


# =============================================================================
# CELL 3: UNIT TESTS - KERNEL FUNCTIONS
# =============================================================================
def test_kernel_functions():
    """Test kernel functions (score and value forward) in isolation."""
    print("\n" + "=" * 70)
    print("UNIT TEST 2: Kernel Functions")
    print("=" * 70)
    
    if not SPECTRAL_OK:
        print("‚ùå Skipping: unsloth_spectral not available")
        return False
    
    if not TRITON_AVAILABLE or spectral_score_forward is None:
        print("‚ö†Ô∏è Triton kernels not available, testing PyTorch fallbacks")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16 if device == "cuda" else torch.float32
    
    # Configuration
    B = 1          # batch
    H_q = 32       # num_attention_heads (query)
    H_kv = 8       # num_key_value_heads
    D = 128        # head_dim
    T_block = 256  # tokens in compressed block
    k_K = 16       # key rank
    k_V = 32       # value rank
    max_seq = 8192 # RoPE table size
    
    print(f"Config: B={B}, H_q={H_q}, H_kv={H_kv}, D={D}, T_block={T_block}")
    
    # Create test tensors
    Q = torch.randn(B, H_q, 1, D, device=device, dtype=dtype)
    coeffs_K = torch.randn(H_kv, T_block, k_K, device=device, dtype=dtype)
    basis_K = torch.randn(H_kv, k_K, D, device=device, dtype=dtype)
    coeffs_V = torch.randn(H_kv, T_block, k_V, device=device, dtype=dtype)
    basis_V = torch.randn(H_kv, k_V, D, device=device, dtype=dtype)
    
    # RoPE tables
    cos = torch.randn(max_seq, D, device=device, dtype=dtype)
    sin = torch.randn(max_seq, D, device=device, dtype=dtype)
    
    config = TritonSpectralConfig(enable_logging=True) if TritonSpectralConfig else None
    
    # Test 1: spectral_score_forward
    print("\n--- Test 1: spectral_score_forward ---")
    if spectral_score_forward is not None:
        scores = spectral_score_forward(
            Q=Q,
            coeffs_K=coeffs_K,
            basis_K=basis_K,
            cos=cos,
            sin=sin,
            start_position=0,
            query_position=256,
            config=config,
            use_triton=False,  # PyTorch for correctness
        )
        print(f"Scores shape: {scores.shape} (expected: [{B}, {H_q}, 1, {T_block}])")
        print(f"Scores dtype: {scores.dtype}")
        print(f"Scores stats: min={scores.min():.4f}, max={scores.max():.4f}, mean={scores.mean():.4f}")
        
        expected_shape = (B, H_q, 1, T_block)
        assert scores.shape == expected_shape, f"Shape mismatch: {scores.shape} vs {expected_shape}"
        assert scores.dtype == dtype, f"Dtype mismatch: {scores.dtype} vs {dtype}"
        print("‚úÖ spectral_score_forward passed")
    else:
        print("‚ö†Ô∏è spectral_score_forward not available")
    
    # Test 2: spectral_value_forward
    print("\n--- Test 2: spectral_value_forward ---")
    if spectral_value_forward is not None:
        # Create attention weights from scores
        if spectral_score_forward is not None:
            scale = 1.0 / math.sqrt(D)
            attn = torch.softmax(scores * scale, dim=-1)  # [B, H_q, 1, T_block]
        else:
            attn = torch.softmax(torch.randn(B, H_q, 1, T_block, device=device, dtype=dtype), dim=-1)
        
        output = spectral_value_forward(
            attn=attn,
            coeffs_V=coeffs_V,
            basis_V=basis_V,
            D=D,
            config=config,
            use_triton=False,  # PyTorch for correctness
        )
        print(f"Output shape: {output.shape} (expected: [{B}, {H_q}, 1, {D}])")
        print(f"Output dtype: {output.dtype}")
        print(f"Output stats: min={output.min():.4f}, max={output.max():.4f}, mean={output.mean():.4f}")
        
        expected_shape = (B, H_q, 1, D)
        assert output.shape == expected_shape, f"Shape mismatch: {output.shape} vs {expected_shape}"
        assert output.dtype == dtype, f"Dtype mismatch: {output.dtype} vs {dtype}"
        print("‚úÖ spectral_value_forward passed")
    else:
        print("‚ö†Ô∏è spectral_value_forward not available")
    
    # Test 3: spectral_attention_decode (full integration)
    print("\n--- Test 3: spectral_attention_decode (integration) ---")
    if spectral_attention_decode is not None:
        # Create a proper SpectralCache with enough data for cold blocks
        cache = SpectralCache(
            num_heads=H_kv,
            head_dim=D,
            block_size=512,
            k_rank_keys=k_K,
            k_rank_values=k_V,
            device=device,
            dtype=dtype,
            debug_logging=False,
        )
        
        # Add enough data to create a cold block
        K_test = torch.randn(B, H_kv, 700, D, device=device, dtype=dtype)
        V_test = torch.randn(B, H_kv, 700, D, device=device, dtype=dtype)
        cache.append(K_test, V_test)
        
        print(f"Cache state: total={cache.total_tokens}, cold_blocks={len(cache.cold_blocks)}")
        
        # Run spectral_attention_decode
        Q_test = torch.randn(B, H_q, 1, D, device=device, dtype=dtype)
        output = spectral_attention_decode(
            Q=Q_test,
            cache=cache,
            cos=cos,
            sin=sin,
            query_position=699,
            attention_mask=None,
            scale=1.0 / math.sqrt(D),
            config=config,
        )
        
        print(f"Decode output shape: {output.shape} (expected: [{B}, {H_q}, 1, {D}])")
        print(f"Decode output dtype: {output.dtype}")
        
        expected_shape = (B, H_q, 1, D)
        assert output.shape == expected_shape, f"Shape mismatch: {output.shape} vs {expected_shape}"
        print("‚úÖ spectral_attention_decode passed")
    else:
        print("‚ö†Ô∏è spectral_attention_decode not available")
    
    print("\n" + "=" * 70)
    print("UNIT TEST 2: ‚úÖ ALL PASSED")
    print("=" * 70)
    return True


# =============================================================================
# CELL 4: LONG CONTEXT TEST - COMPRESSION VERIFICATION
# =============================================================================
def test_long_context_compression():
    """Test that compression actually triggers and works with long contexts."""
    print("\n" + "=" * 70)
    print("LONG CONTEXT TEST: Compression Verification")
    print("=" * 70)
    
    if not (SPECTRAL_OK and UNSLOTH_OK):
        print("‚ùå Skipping: Required modules not available")
        return False
    
    if not torch.cuda.is_available():
        print("‚ùå Skipping: CUDA not available")
        return False
    
    # Load model
    print("\nüì• Loading model...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
        max_seq_length=4096,
        load_in_4bit=True,
    )
    
    # Patch with spectral cache
    print("\nüîß Patching with SpectralCache...")
    patch_unsloth_attention(
        model,
        block_size=512,
        k_rank_keys=16,
        k_rank_values=32,
        hot_buffer_size=64,
        use_spectral_attention=True,
        verbose=True,
        debug_logging=False,
        use_triton_kernel=True,
    )
    
    # Create a LONG prompt (> 512 tokens to trigger compression)
    print("\nüìù Creating long prompt (target: >600 tokens)...")
    
    # Generate a long context with repeated pattern
    base_text = """
    This is an extensive technical document about machine learning. 
    Machine learning involves training models on data to make predictions.
    Neural networks are a key component of modern machine learning systems.
    Deep learning has revolutionized computer vision, NLP, and more.
    Transformers have become the dominant architecture for language tasks.
    """
    
    # Repeat to get >600 tokens
    long_context = (base_text * 15)  # Should be ~750 tokens
    
    # Add a specific fact we'll test recall on
    needle = "The secret code is ALPHA-7892."
    long_context += f"\n\nIMPORTANT NOTE: {needle}\n\n"
    long_context += "Now, continuing with our discussion of transformers..."
    
    # Tokenize
    inputs = tokenizer(long_context, return_tensors="pt").to("cuda")
    prompt_length = inputs["input_ids"].shape[1]
    print(f"Prompt length: {prompt_length} tokens")
    
    if prompt_length < 512:
        print(f"‚ö†Ô∏è Warning: Prompt ({prompt_length}) < block_size (512). Adding more content...")
        long_context = (base_text * 25) + f"\n\nIMPORTANT NOTE: {needle}\n\n"
        inputs = tokenizer(long_context, return_tensors="pt").to("cuda")
        prompt_length = inputs["input_ids"].shape[1]
        print(f"Extended prompt length: {prompt_length} tokens")
    
    # Generate with use_cache=True
    print("\nüîÑ Running generation...")
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            do_sample=False,
            use_cache=True,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
        )
    
    gen_time = time.time() - start_time
    total_tokens = outputs.shape[1]
    new_tokens = total_tokens - prompt_length
    
    print(f"Generation time: {gen_time:.2f}s")
    print(f"Generated {new_tokens} new tokens")
    print(f"Speed: {new_tokens / gen_time:.2f} tokens/sec")
    
    # Decode output
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nGenerated text (last 200 chars): ...{output_text[-200:]}")
    
    # Check cache statistics
    print("\nüìä Cache Statistics:")
    stats = get_cache_stats(model)
    print(f"  Layers with cache: {stats['layers_with_cache']}")
    print(f"  Total tokens: {stats['total_tokens']}")
    print(f"  Total blocks: {stats['total_blocks']}")
    print(f"  Compression ratio: {stats['compression_ratio']:.2f}x")
    
    # Verify compression occurred
    success = True
    if prompt_length > 512:
        expected_blocks = prompt_length // 512
        if stats['total_blocks'] == 0:
            print(f"‚ùå FAIL: Expected compression (prompt={prompt_length} > 512) but got 0 blocks")
            success = False
        else:
            print(f"‚úÖ Compression verified: {stats['total_blocks']} blocks for {prompt_length} tokens")
    
    # Test needle recall
    print("\nüîç Testing needle recall...")
    recall_prompt = long_context + "\nWhat is the secret code mentioned above?"
    recall_inputs = tokenizer(recall_prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        recall_outputs = model.generate(
            **recall_inputs,
            max_new_tokens=30,
            do_sample=False,
            use_cache=True,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
        )
    
    recall_text = tokenizer.decode(recall_outputs[0][recall_inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    print(f"Recall response: {recall_text}")
    
    if "ALPHA-7892" in recall_text or "alpha-7892" in recall_text.lower():
        print("‚úÖ Needle successfully recalled!")
    else:
        print("‚ö†Ô∏è Needle not found in response (may be model limitation)")
    
    # Cleanup
    del model, tokenizer
    gc.collect()
    torch.cuda.empty_cache()
    
    print("\n" + "=" * 70)
    print(f"LONG CONTEXT TEST: {'‚úÖ PASSED' if success else '‚ùå FAILED'}")
    print("=" * 70)
    return success


# =============================================================================
# CELL 5: MEMORY PROFILING
# =============================================================================
def test_memory_usage():
    """Profile VRAM usage with and without spectral compression."""
    print("\n" + "=" * 70)
    print("MEMORY PROFILING")
    print("=" * 70)
    
    if not (SPECTRAL_OK and UNSLOTH_OK):
        print("‚ùå Skipping: Required modules not available")
        return False
    
    if not torch.cuda.is_available():
        print("‚ùå Skipping: CUDA not available")
        return False
    
    def get_memory_mb():
        torch.cuda.synchronize()
        return torch.cuda.memory_allocated() / 1024**2
    
    def get_max_memory_mb():
        return torch.cuda.max_memory_allocated() / 1024**2
    
    # Reset memory tracking
    torch.cuda.reset_peak_memory_stats()
    gc.collect()
    torch.cuda.empty_cache()
    
    baseline_memory = get_memory_mb()
    print(f"Baseline VRAM: {baseline_memory:.1f} MB")
    
    # Load model
    print("\nüì• Loading model...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
        max_seq_length=4096,
        load_in_4bit=True,
    )
    
    model_memory = get_memory_mb()
    print(f"After model load: {model_memory:.1f} MB (+{model_memory - baseline_memory:.1f} MB)")
    
    # Patch with spectral cache
    patch_unsloth_attention(
        model,
        block_size=512,
        k_rank_keys=16,
        k_rank_values=32,
        verbose=True,
        debug_logging=False,
    )
    
    # Create long prompt
    long_text = "The quick brown fox jumps over the lazy dog. " * 150  # ~900 tokens
    inputs = tokenizer(long_text, return_tensors="pt").to("cuda")
    prompt_tokens = inputs["input_ids"].shape[1]
    print(f"\nPrompt length: {prompt_tokens} tokens")
    
    # Pre-generation memory
    pre_gen_memory = get_memory_mb()
    print(f"Pre-generation: {pre_gen_memory:.1f} MB")
    
    # Run generation
    torch.cuda.reset_peak_memory_stats()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=False,
            use_cache=True,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
        )
    
    post_gen_memory = get_memory_mb()
    peak_memory = get_max_memory_mb()
    
    print(f"\nPost-generation: {post_gen_memory:.1f} MB")
    print(f"Peak during generation: {peak_memory:.1f} MB")
    print(f"Generation overhead: {peak_memory - pre_gen_memory:.1f} MB")
    
    # Get cache stats
    stats = get_cache_stats(model)
    print(f"\nCache statistics:")
    print(f"  Total tokens cached: {stats['total_tokens']}")
    print(f"  Compression ratio: {stats['compression_ratio']:.2f}x")
    
    # Theoretical uncompressed size
    # Assuming 32 layers, 8 KV heads, 128 dim, FP16
    theoretical_uncompressed_mb = (
        32 * 8 * stats['total_tokens'] * 128 * 2 * 2  # K and V
    ) / 1024**2 if stats['total_tokens'] > 0 else 0
    
    print(f"\nüìä Memory Analysis:")
    print(f"  Theoretical uncompressed KV cache: {theoretical_uncompressed_mb:.1f} MB")
    print(f"  Actual compressed: {stats.get('total_compressed_bytes', 0) / 1024**2:.1f} MB")
    print(f"  Compression achieved: {stats['compression_ratio']:.2f}x")
    
    # Cleanup
    del model, tokenizer
    gc.collect()
    torch.cuda.empty_cache()
    
    print("\n" + "=" * 70)
    print("MEMORY PROFILING: ‚úÖ COMPLETE")
    print("=" * 70)
    return True


# =============================================================================
# CELL 6: QUALITY COMPARISON
# =============================================================================
def test_generation_quality():
    """Compare generation quality with and without spectral compression."""
    print("\n" + "=" * 70)
    print("QUALITY COMPARISON TEST")
    print("=" * 70)
    
    if not (SPECTRAL_OK and UNSLOTH_OK):
        print("‚ùå Skipping: Required modules not available")
        return False
    
    if not torch.cuda.is_available():
        print("‚ùå Skipping: CUDA not available")
        return False
    
    # Load model WITH spectral cache
    print("\nüì• Loading model with spectral cache...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
        max_seq_length=4096,
        load_in_4bit=True,
    )
    
    patch_unsloth_attention(
        model,
        block_size=512,
        k_rank_keys=16,
        k_rank_values=32,
        verbose=True,
        debug_logging=False,
    )
    
    # Test prompts
    prompts = [
        "Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144,",
        "The quick brown fox",
        "What is 15 + 27?",
        "Write a haiku about programming:",
    ]
    
    print("\nüìù Generating with spectral cache...")
    spectral_outputs = []
    
    for prompt in prompts:
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=False,
                use_cache=True,
                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
            )
        
        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        spectral_outputs.append(text)
        print(f"\nPrompt: {prompt}")
        print(f"Output: {text[len(prompt):].strip()[:100]}...")
    
    # Check specific outputs
    print("\n" + "=" * 70)
    print("QUALITY ANALYSIS")
    print("=" * 70)
    
    # Fibonacci check
    fib_output = spectral_outputs[0]
    if any(n in fib_output for n in ["233", "377", "610"]):
        print("‚úÖ Fibonacci continuation: CORRECT (found expected numbers)")
    else:
        print("‚ö†Ô∏è Fibonacci continuation: May be incorrect")
    
    # Fox check
    fox_output = spectral_outputs[1]
    if "jumps" in fox_output.lower() or "lazy" in fox_output.lower():
        print("‚úÖ Common phrase completion: CORRECT")
    else:
        print("‚ö†Ô∏è Common phrase completion: Unexpected output")
    
    # Math check
    math_output = spectral_outputs[2]
    if "42" in math_output:
        print("‚úÖ Math problem: CORRECT")
    else:
        print(f"‚ö†Ô∏è Math problem: Got '{math_output}', expected '42'")
    
    # Cleanup
    del model, tokenizer
    gc.collect()
    torch.cuda.empty_cache()
    
    print("\n" + "=" * 70)
    print("QUALITY COMPARISON: ‚úÖ COMPLETE")
    print("=" * 70)
    return True


# =============================================================================
# CELL 7: PERFORMANCE BENCHMARK
# =============================================================================
def test_performance_benchmark():
    """Benchmark generation speed with spectral cache."""
    print("\n" + "=" * 70)
    print("PERFORMANCE BENCHMARK")
    print("=" * 70)
    
    if not (SPECTRAL_OK and UNSLOTH_OK):
        print("‚ùå Skipping: Required modules not available")
        return False
    
    if not torch.cuda.is_available():
        print("‚ùå Skipping: CUDA not available")
        return False
    
    # Load model
    print("\nüì• Loading model...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
        max_seq_length=4096,
        load_in_4bit=True,
    )
    
    patch_unsloth_attention(
        model,
        block_size=512,
        k_rank_keys=16,
        k_rank_values=32,
        verbose=True,
        debug_logging=False,
    )
    
    # Warmup
    print("\nüî• Warmup...")
    warmup_input = tokenizer("Hello", return_tensors="pt").to("cuda")
    with torch.no_grad():
        _ = model.generate(**warmup_input, max_new_tokens=10, use_cache=True, 
                          pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id)
    torch.cuda.synchronize()
    
    # Benchmark configurations
    configs = [
        {"name": "Short (100 tokens)", "prompt_tokens": 100, "new_tokens": 50},
        {"name": "Medium (500 tokens)", "prompt_tokens": 500, "new_tokens": 50},
        {"name": "Long (1000 tokens)", "prompt_tokens": 1000, "new_tokens": 50},
    ]
    
    print("\nüìä Running benchmarks...")
    print("-" * 60)
    
    for config in configs:
        # Create prompt of specified length
        base = "This is a test sentence for benchmarking. "
        prompt = base * (config["prompt_tokens"] // 10 + 1)
        inputs = tokenizer(prompt, return_tensors="pt", max_length=config["prompt_tokens"], 
                          truncation=True).to("cuda")
        actual_prompt_len = inputs["input_ids"].shape[1]
        
        # Run benchmark (3 iterations)
        times = []
        for _ in range(3):
            torch.cuda.synchronize()
            start = time.time()
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=config["new_tokens"],
                    do_sample=False,
                    use_cache=True,
                    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
                )
            
            torch.cuda.synchronize()
            times.append(time.time() - start)
        
        avg_time = sum(times) / len(times)
        tokens_per_sec = config["new_tokens"] / avg_time
        
        # Get compression stats
        stats = get_cache_stats(model)
        
        print(f"\n{config['name']}:")
        print(f"  Prompt: {actual_prompt_len} tokens")
        print(f"  Generated: {config['new_tokens']} tokens")
        print(f"  Time: {avg_time:.3f}s (avg of 3)")
        print(f"  Speed: {tokens_per_sec:.1f} tokens/sec")
        print(f"  Cache compression: {stats['compression_ratio']:.2f}x")
        print(f"  Cold blocks: {stats['total_blocks']}")
    
    # Cleanup
    del model, tokenizer
    gc.collect()
    torch.cuda.empty_cache()
    
    print("\n" + "=" * 70)
    print("PERFORMANCE BENCHMARK: ‚úÖ COMPLETE")
    print("=" * 70)
    return True


# =============================================================================
# MAIN EXECUTION
# =============================================================================
def run_all_tests():
    """Run all validation tests."""
    print("\n" + "=" * 70)
    print("üöÄ RUNNING ALL VALIDATION TESTS")
    print("=" * 70)
    
    results = {}
    
    # Unit tests (always run)
    results["SpectralCache Unit"] = test_spectral_cache_unit()
    results["Kernel Functions"] = test_kernel_functions()
    
    # Integration tests (require full setup)
    results["Long Context Compression"] = test_long_context_compression()
    results["Memory Profiling"] = test_memory_usage()
    results["Generation Quality"] = test_generation_quality()
    results["Performance Benchmark"] = test_performance_benchmark()
    
    # Summary
    print("\n" + "=" * 70)
    print("üìã TEST SUMMARY")
    print("=" * 70)
    
    passed = sum(1 for v in results.values() if v)
    total = len(results)
    
    for name, result in results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"  {name}: {status}")
    
    print("-" * 70)
    print(f"  Total: {passed}/{total} tests passed")
    print("=" * 70)
    
    return passed == total


# Run all tests when executed
if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
else:
    # When pasted into Colab, run automatically
    success = run_all_tests()
