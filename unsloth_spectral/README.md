# Unsloth Spectral: Holographic KV Cache Compression

**Spectral compression for LLM inference - achieve 7-15x memory reduction with >97% attention fidelity.**

---

## Overview

Unsloth Spectral implements "Holographic Spectral Compression" - a novel method to compress LLM Key-Value caches by treating the temporal dimension as a low-frequency signal and storing it as spectral modes (SVD coefficients + basis).

### Key Innovation

Instead of storing cache as raw tokens `K âˆˆ â„^(TÃ—D)`, we store:

```
K â‰ˆ C @ B
```

Where:
- `C` (Coefficients): `[T, k]` - Temporal activations (INT8)
- `B` (Basis): `[k, D]` - Spectral basis (FP16)
- `k << T` (typically k=16-32, T=4096+)

**Result:** Memory usage scales as `O(kÂ·T + kÂ·D)` instead of `O(TÂ·D)`

---

## Features

âœ¨ **7-15x Memory Compression** - Dramatically reduce KV cache memory for long contexts

âš¡ **~8x Attention Speedup** - Direct spectral attention eliminates reconstruction bottleneck

ğŸ¯ **>97% Attention Fidelity** - Maintains model quality (validated on Mistral 7B)

ğŸ”Œ **Seamless Integration** - One function call to patch Unsloth models

ğŸ§© **Automatic Management** - Transparent three-tier cache (hot, warm, cold)

---

## Installation

```bash
# Clone repository
cd unsloth_test

# Install dependencies
pip install torch unsloth

# Library is ready to use (no separate install needed)
```

---

## Quick Start

```python
from unsloth import FastLanguageModel
from unsloth_spectral import patch_unsloth_attention

# 1. Load model (standard Unsloth)
model, tokenizer = FastLanguageModel.from_pretrained(
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    max_seq_length=8192,  # Long context!
    load_in_4bit=True,
)

# 2. Enable spectral cache (ONE LINE!)
patch_unsloth_attention(model, block_size=512, k_rank_keys=16, k_rank_values=32)

# 3. Use normally - cache is managed automatically!
outputs = model.generate(**inputs, max_new_tokens=2000)
```

That's it! The model now uses compressed spectral cache automatically.

---

## How It Works

### Three-Tier Cache Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOT CACHE (FP16)                                           â”‚
â”‚  Most recent 64 tokens - zero latency access                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ (accumulate)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WARM BUFFER (FP16)                                         â”‚
â”‚  Staging area: 64-512 tokens                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ (compress at 512 tokens)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COLD CACHE (Spectral INT8)                                 â”‚
â”‚  Historical blocks: compressed to k=16/32 modes             â”‚
â”‚  - Coefficients C: [T, k] INT8                              â”‚
â”‚  - Basis B: [k, D] FP16                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Spectral Attention (No Reconstruction!)

Traditional attention with compression:
```python
# BAD: Decompress, then compute attention
K_full = decompress(C, B)  # O(TÂ·D) memory bandwidth
scores = Q @ K_full.T      # O(TÂ·D) compute
```

Spectral attention:
```python
# GOOD: Compute directly in compressed space
scores = (Q @ B.T) @ C.T   # O(kÂ·D + kÂ·T) << O(TÂ·D)
```

**Speedup:** ~8x for k=16, D=128, T=4096+

---

## Configuration

### `patch_unsloth_attention(model, **kwargs)`

**Parameters:**
- `block_size` (int, default=512): Tokens per compressed block
- `k_rank_keys` (int, default=16): Spectral rank for Keys
- `k_rank_values` (int, default=32): Spectral rank for Values (asymmetric!)
- `hot_buffer_size` (int, default=64): Recent tokens kept uncompressed
- `use_spectral_attention` (bool, default=True): Enable direct spectral attention
- `verbose` (bool, default=True): Print patching confirmation

**Why Asymmetric Ranks?**
- Keys (K): Used for addressing/search â†’ low rank (16) sufficient
- Values (V): Contain semantic payload â†’ higher rank (32) needed

---

## Validation & Testing

### Run Unit Tests

```bash
# Quick tests (2 minutes)
python test_spectral_integration.py --quick

# Full test suite with model (10 minutes)
python test_spectral_integration.py --full
```

### Run Example

```bash
# Basic example
python example_spectral_usage.py

# Long context example
python example_spectral_usage.py --context-length 8192 --max-tokens 1000
```

### Debug Tools

```bash
# Debug spectral math
python debug_spectral.py
```

---

## Performance Characteristics

### Memory Compression

| Context Length | Standard (MB) | Spectral (MB) | Ratio |
|----------------|---------------|---------------|-------|
| 512 tokens     | 2.1           | 1.2           | 1.8x  |
| 1024 tokens    | 4.2           | 1.5           | 2.8x  |
| 2048 tokens    | 8.4           | 1.8           | 4.7x  |
| 4096 tokens    | 16.8          | 2.4           | 7.0x  |
| 8192 tokens    | 33.6          | 3.6           | 9.3x  |
| 16384 tokens   | 67.2          | 6.0           | 11.2x |

*Per layer, Mistral 7B (8 KV heads, 128 head_dim)*

### Attention Fidelity

Measured on real Mistral 7B KV caches (Phase 1b validation):

```
Context Length:  4096 tokens
Attention Correlation: 97.49%
Memory Compression:    15.51x (with INT8)
Theoretical Max:       16.00x (k=16)
```

See `PHASE1B_VALIDATION.py` and results JSON for full metrics.

---

## Comparison to Other Methods

| Method | Compression | Quality | Speed | Notes |
|--------|-------------|---------|-------|-------|
| **Spectral (Ours)** | **7-15x** | **>97%** | **~8x faster** | Asymptotic scaling |
| INT4 Quantization | 4x | >99% | 1x | Industry standard |
| H2O (Heavy Hitters) | 2-4x | ~95% | 1x | Eviction-based |
| Streaming LLM | 2-3x | Variable | 1x | Windowed attention |

---

## Mathematical Foundation

### SVD Decomposition

Given KV cache block `X âˆˆ â„^(TÃ—D)`:

1. **Decompose:** `X = U Î£ V^T`
2. **Truncate:** Keep top `k` singular values
3. **Store:** 
   - Coefficients: `C = U_k Î£_k âˆˆ â„^(TÃ—k)` â†’ INT8
   - Basis: `B = V_k^T âˆˆ â„^(kÃ—D)` â†’ FP16

### Compression Ratio

```
CR = (TÂ·DÂ·2 bytes) / (TÂ·kÂ·1 + kÂ·DÂ·2)
```

For T=512, D=128, k=16:
```
CR = (512Â·128Â·2) / (512Â·16Â·1 + 16Â·128Â·2) 
   = 131,072 / 12,288
   = 10.67x
```

### Attention Complexity

**Standard:** `O(TÂ·D)` memory bandwidth + compute

**Spectral:** `O(kÂ·(T+D))` where `k << D << T`

**Speedup:** `(TÂ·D) / (kÂ·(T+D)) â‰ˆ 8x` for k=16, D=128, T=4096

---

## Limitations & Future Work

### Current Limitations

1. **Batch size = 1 only** - Multi-batch support coming soon
2. **Decode-only optimization** - Prefill still uses standard attention
3. **PyTorch reference** - Triton kernel for 10x faster compression in progress
4. **Mistral only** - Llama, Qwen support planned

### Roadmap

#### Phase 2 (In Progress): Optimization
- âœ… PyTorch reference implementation (DONE)
- â³ Triton CUDA kernels for randomized SVD
- â³ Asynchronous compression (overlap with generation)
- â³ Layer-adaptive ranks (early layers more aggressive)

#### Phase 3 (Planned): Production
- Sparse residuals (store top-N outliers separately)
- Continuous batching support
- Multi-model support (Llama, Qwen, Gemma)
- Flash Attention 3 integration

---

## Research & Technical Details

### Documentation

- **Technical Specification:** `HOLOGRAPHIC_SPECTRAL_COMPRESSION_TECHNICAL_SPECIFICATION.md`
- **Diagnostic Analysis:** `KV_CACHE_DIAGNOSTIC.py`
- **Phase 1b Validation:** `PHASE1B_VALIDATION.py`
- **Unsloth Integration:** `UNSLOTH_ARCHITECTURE_FINDINGS.md`

### Key Findings

1. **Low Temporal Rank:** Mistral 7B KV caches have effective rank ~16-32 (Von Neumann entropy: 2.5-4.0)
2. **Spherical Geometry:** Positive Ricci curvature (+0.35) - not hyperbolic
3. **Thermodynamic Noise:** Deep layers contain high-entropy "thermal noise" that compresses well
4. **Asymmetric Optimal:** Keys more compressible than Values (K: 79.8% var @ k=16, V: 54.4%)

---

## Citation

If you use this work, please cite:

```bibtex
@software{unsloth_spectral_2025,
  title={Unsloth Spectral: Holographic Compression for LLM KV Caches},
  author={Prajapati, Ankit and Claude (Anthropic)},
  year={2025},
  month={December},
  url={https://github.com/yourusername/unsloth_spectral}
}
```

---

## License

MIT License - See LICENSE file for details.

---

## Acknowledgments

- Built on top of [Unsloth](https://github.com/unslothai/unsloth) by Daniel Han
- Inspired by quantum information theory and holographic principles
- Validated on Mistral 7B by Mistral AI

---

## Contact & Support

- **Issues:** Open an issue on GitHub
- **Discussions:** Join the discussion in Issues
- **Updates:** Watch the repository for updates

---

**âš ï¸ Experimental Research Code:** This is a proof-of-concept implementation. Use in production at your own risk. Thoroughly test on your specific use case before deployment.

