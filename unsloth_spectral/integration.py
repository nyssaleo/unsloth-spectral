"""
Unsloth Integration: Monkey-patching for Spectral Cache

This module provides seamless integration with Unsloth's optimized Mistral implementation.
It monkey-patches the MistralAttention_fast_forward method to use SpectralCache instead
of standard tuple-based KV cache.

Key Design: The patched forward method is a drop-in replacement that:
1. Detects if past_key_value is a SpectralCache
2. Converts tuple caches to SpectralCache automatically
3. Uses spectral_attention_forward for computation
4. Returns compatible outputs for model.generate()
"""

import torch
import torch.nn.functional as F
from typing import Optional, Tuple
from .spectral_cache import SpectralCache
from .spectral_attention import spectral_attention_forward
import math


def create_spectral_forward(
    original_forward,
    block_size: int = 512,
    k_rank_keys: int = 16,
    k_rank_values: int = 32,
    hot_buffer_size: int = 64,
    use_spectral_attention: bool = True,
    debug_logging: bool = False,
):
    """
    Factory function that creates a spectral-enabled forward method.
    
    This wraps the original MistralAttention forward to inject spectral cache logic.
    
    Args:
        original_forward: Original self.forward method
        block_size: Tokens per compressed block
        k_rank_keys: Spectral rank for Keys
        k_rank_values: Spectral rank for Values
        hot_buffer_size: Number of recent tokens kept uncompressed
        use_spectral_attention: If True, use direct spectral attention (no reconstruction)
        
    Returns:
        spectral_forward: Modified forward method with spectral cache
    """
    
    def spectral_forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        """
        Modified forward pass with spectral cache integration.
        
        This method:
        1. Computes Q, K, V projections (standard)
        2. Applies RoPE to Q, K (standard)
        3. Manages SpectralCache (new)
        4. Computes attention using spectral method (new)
        5. Returns output in Unsloth-compatible format (standard)
        """
        
        # Get architecture parameters (compatible with Unsloth/Mistral)
        # Mistral stores these in config, not directly on attention module
        if hasattr(self, 'config'):
            num_heads = self.config.num_attention_heads
            num_key_value_heads = self.config.num_key_value_heads
            head_dim = self.config.hidden_size // num_heads
            hidden_size = self.config.hidden_size
            num_key_value_groups = num_heads // num_key_value_heads
        else:
            # Fallback for other architectures
            num_heads = self.num_heads
            num_key_value_heads = self.num_key_value_heads
            head_dim = self.head_dim
            hidden_size = self.hidden_size
            num_key_value_groups = self.num_key_value_groups
        
        bsz, q_len, _ = hidden_states.size()
        
        # 1. QKV Projection (Standard Unsloth path)
        # ============================================
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)
        
        # Reshape to [B, H, T, D]
        query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)
        
        if debug_logging:
            print(f"[SpectralForward] After QKV projection:")
            print(f"  Q shape: {query_states.shape} (num_heads={num_heads})")
            print(f"  K shape: {key_states.shape} (num_kv_heads={num_key_value_heads})")
            print(f"  V shape: {value_states.shape} (num_kv_heads={num_key_value_heads})")
        
        # 2. RoPE (Rotary Positional Embeddings) - Unsloth's method
        # ===========================================================
        # Calculate total sequence length (current + past cache)
        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            if isinstance(past_key_value, SpectralCache):
                kv_seq_len += past_key_value.total_tokens
            elif isinstance(past_key_value, (tuple, list)) and len(past_key_value) == 2:
                kv_seq_len += past_key_value[0].shape[-2]
        
        if debug_logging:
            print(f"[SpectralForward] RoPE: kv_seq_len={kv_seq_len}")
        
        # Extend RoPE embedding cache if needed
        if hasattr(self, 'rotary_emb') and hasattr(self.rotary_emb, 'extend_rope_embedding'):
            # Unsloth's optimized RoPE
            self.rotary_emb.extend_rope_embedding(value_states, seq_len=kv_seq_len)
            cos, sin = self.rotary_emb.get_cached(kv_seq_len, query_states.device.index)
            
            # Use Unsloth's fast_rope_embedding if available
            try:
                from unsloth.models.llama import fast_rope_embedding
                query_states, key_states = fast_rope_embedding(query_states, key_states, cos, sin, position_ids)
            except ImportError:
                # Fallback to standard RoPE
                query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
        else:
            # Fallback for non-Unsloth models (standard transformers)
            cos, sin = self.rotary_emb(value_states, position_ids)
            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
        
        if debug_logging:
            print(f"[SpectralForward] After RoPE:")
            print(f"  Q shape: {query_states.shape}")
            print(f"  K shape: {key_states.shape}")
        
        # 3. Spectral Cache Management - CRITICAL: Store BEFORE GQA expansion!
        # =====================================================================
        
        # Check if past_key_value is already a SpectralCache
        if not isinstance(past_key_value, SpectralCache):
            # First call or tuple-based cache - create SpectralCache
            # CRITICAL: Use num_key_value_heads, NOT num_heads!
            cache = SpectralCache(
                num_heads=num_key_value_heads,  # FIXED: Was num_heads (32), should be num_kv_heads (8)
                head_dim=head_dim,
                block_size=block_size,
                k_rank_keys=k_rank_keys,
                k_rank_values=k_rank_values,
                hot_buffer_size=hot_buffer_size,
                device=hidden_states.device,
                dtype=hidden_states.dtype,
            )
            
            # If there was a previous tuple cache, initialize with it
            if past_key_value is not None and isinstance(past_key_value, (tuple, list)):
                if len(past_key_value) == 2:
                    past_K, past_V = past_key_value
                    if debug_logging:
                        print(f"[SpectralForward] Initializing cache with past K/V:")
                        print(f"  Past K shape: {past_K.shape}")
                        print(f"  Past V shape: {past_V.shape}")
                    cache.append(past_K, past_V)
            
            past_key_value = cache
        
        # CRITICAL BUG FIX: Append to cache BEFORE repeat_kv expansion!
        # Unsloth expects cache to store [B, num_kv_heads, T, D], not [B, num_heads, T, D]
        if debug_logging:
            print(f"[SpectralForward] Appending to cache (BEFORE repeat_kv):")
            print(f"  K shape: {key_states.shape} (num_kv_heads={num_key_value_heads})")
            print(f"  V shape: {value_states.shape}")
        
        past_key_value.append(key_states, value_states)
        
        # 4. Attention Computation
        # ==========================
        # NOTE: GQA expansion happens AFTER retrieving full context from cache,
        # not here! We need to expand the COMPLETE history (past + current),
        # not just the current tokens.
        
        if debug_logging:
            print(f"[SpectralForward] Before attention:")
            print(f"  Q shape: {query_states.shape}")
            print(f"  Cache total tokens: {past_key_value.total_tokens}")
            print(f"  Note: K/V will be retrieved and expanded from cache during attention")
        
        if use_spectral_attention and past_key_value.total_tokens > block_size:
            # Use spectral attention (no reconstruction)
            # This is the core optimization!
            
            # Note: spectral_attention currently only supports single-token decode
            # For prefill (q_len > 1), we fall back to standard attention
            if q_len == 1:
                attn_output = spectral_attention_forward(
                    Q=query_states,
                    cache=past_key_value,
                    attention_mask=attention_mask,
                    scale=1.0 / math.sqrt(head_dim),
                )
            else:
                # Prefill: use standard attention with reconstructed K/V
                K_full, V_full = past_key_value.get_kv()
                
                # CRITICAL FIX: Expand K/V to match Q heads AFTER getting full context
                if num_key_value_groups > 1:
                    if debug_logging:
                        print(f"[SpectralForward] Expanding K_full/V_full for GQA:")
                        print(f"  Before: K_full {K_full.shape}, V_full {V_full.shape}")
                    K_full = repeat_kv(K_full, num_key_value_groups)
                    V_full = repeat_kv(V_full, num_key_value_groups)
                    if debug_logging:
                        print(f"  After: K_full {K_full.shape}, V_full {V_full.shape}")
                
                attn_weights = torch.matmul(query_states, K_full.transpose(2, 3)) / math.sqrt(head_dim)
                
                if attention_mask is not None:
                    attn_weights = attn_weights + attention_mask
                
                attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
                attn_output = torch.matmul(attn_weights, V_full)
        else:
            # Standard attention (for short contexts or validation)
            K_full, V_full = past_key_value.get_kv()
            
            # CRITICAL FIX: Expand K/V to match Q heads AFTER getting full context
            if num_key_value_groups > 1:
                if debug_logging:
                    print(f"[SpectralForward] Expanding K_full/V_full for GQA (standard path):")
                    print(f"  Before: K_full {K_full.shape}, V_full {V_full.shape}")
                K_full = repeat_kv(K_full, num_key_value_groups)
                V_full = repeat_kv(V_full, num_key_value_groups)
                if debug_logging:
                    print(f"  After: K_full {K_full.shape}, V_full {V_full.shape}")
            
            attn_weights = torch.matmul(query_states, K_full.transpose(2, 3)) / math.sqrt(head_dim)
            
            if attention_mask is not None:
                attn_weights = attn_weights + attention_mask
            
            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
            attn_output = torch.matmul(attn_weights, V_full)
        
        # 5. Output Projection (Standard)
        # =================================
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, hidden_size)
        attn_output = self.o_proj(attn_output)
        
        # Store cache reference for get_cache_stats()
        if use_cache and isinstance(past_key_value, SpectralCache):
            self._spectral_cache = past_key_value
        
        return attn_output, None, past_key_value if use_cache else None
    
    return spectral_forward


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None):
    """
    Apply rotary positional embeddings (RoPE).
    
    Fallback implementation for non-Unsloth models.
    Unsloth models should use fast_rope_embedding instead.
    """
    # Unsqueeze for broadcasting if needed
    # cos, sin shape: [seq_len, head_dim] or [batch, seq_len, head_dim]
    # q, k shape: [batch, num_heads, seq_len, head_dim]
    
    # Handle position_ids if provided
    if position_ids is not None:
        # Select the appropriate cos/sin values for each position
        # Reshape position_ids to index into cos/sin
        # This is a simplified version - actual implementation may vary
        pass
    
    # Ensure cos and sin have the right shape
    if cos.dim() == 2:
        # [seq_len, head_dim] -> [1, 1, seq_len, head_dim]
        cos = cos.unsqueeze(0).unsqueeze(0)
        sin = sin.unsqueeze(0).unsqueeze(0)
    elif cos.dim() == 3:
        # [batch, seq_len, head_dim] -> [batch, 1, seq_len, head_dim]
        cos = cos.unsqueeze(1)
        sin = sin.unsqueeze(1)
    
    # Rotate half dimensions
    def rotate_half(x):
        x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
        return torch.cat((-x2, x1), dim=-1)
    
    # Apply rotation
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    Repeat K/V heads for Grouped Query Attention (GQA).
    
    This is equivalent to torch.repeat_interleave(x, dim=1, repeats=n_rep).
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def patch_unsloth_attention(
    model,
    block_size: int = 512,
    k_rank_keys: int = 16,
    k_rank_values: int = 32,
    hot_buffer_size: int = 64,
    use_spectral_attention: bool = True,
    verbose: bool = True,
    debug_logging: bool = False,
):
    """
    Monkey-patch all attention layers in an Unsloth model to use SpectralCache.
    
    This modifies the model in-place, replacing the forward methods of all
    MistralAttention layers with spectral-enabled versions.
    
    Args:
        model: Unsloth FastLanguageModel
        block_size: Tokens per compressed block
        k_rank_keys: Spectral rank for Keys
        k_rank_values: Spectral rank for Values  
        hot_buffer_size: Recent tokens kept uncompressed
        use_spectral_attention: Use direct spectral attention (recommended: True)
        verbose: Print patching confirmation
        
    Returns:
        model: Modified model (in-place)
    """
    
    num_patched = 0
    
    # Patch all transformer layers
    for layer_idx, layer in enumerate(model.model.layers):
        if hasattr(layer, 'self_attn'):
            # Get the original forward method
            original_forward = layer.self_attn.forward
            
            # Create spectral version
            spectral_fwd = create_spectral_forward(
                original_forward=original_forward,
                block_size=block_size,
                k_rank_keys=k_rank_keys,
                k_rank_values=k_rank_values,
                hot_buffer_size=hot_buffer_size,
                use_spectral_attention=use_spectral_attention,
                debug_logging=debug_logging,
            )
            
            # Monkey-patch (bind to instance)
            layer.self_attn.forward = spectral_fwd.__get__(layer.self_attn, type(layer.self_attn))
            num_patched += 1
    
    if verbose:
        print(f"âœ… Patched {num_patched} attention layers with SpectralCache")
        print(f"   Config: block_size={block_size}, k_K={k_rank_keys}, k_V={k_rank_values}")
        print(f"   Spectral Attention: {'Enabled' if use_spectral_attention else 'Disabled (validation mode)'}")
    
    return model


def get_cache_stats(model):
    """
    Extract cache statistics from all layers that have been used.
    
    Note: Only collects stats from layers that have processed at least one forward pass
    with spectral cache enabled.
    
    Returns:
        dict: Aggregated cache statistics including:
            - total_tokens: Total tokens across all layers
            - total_blocks: Total compressed blocks
            - compression_ratio: Average compression ratio
            - per_layer_stats: List of per-layer statistics
    """
    total_tokens = 0
    total_blocks = 0
    total_original_bytes = 0
    total_compressed_bytes = 0
    per_layer_stats = []
    
    for layer_idx, layer in enumerate(model.model.layers):
        if hasattr(layer, 'self_attn') and hasattr(layer.self_attn, '_spectral_cache'):
            cache = layer.self_attn._spectral_cache
            if isinstance(cache, SpectralCache):
                stats = cache.get_memory_stats()
                
                total_tokens += stats['total_tokens']
                total_blocks += stats['num_blocks']
                total_original_bytes += stats['original_bytes']
                total_compressed_bytes += stats['compressed_bytes']
                
                per_layer_stats.append({
                    'layer_idx': layer_idx,
                    'tokens': stats['total_tokens'],
                    'blocks': stats['num_blocks'],
                    'compression': stats['compression_ratio'],
                })
    
    return {
        "total_tokens": total_tokens,
        "total_blocks": total_blocks,
        "total_original_bytes": total_original_bytes,
        "total_compressed_bytes": total_compressed_bytes,
        "compression_ratio": total_original_bytes / total_compressed_bytes if total_compressed_bytes > 0 else 1.0,
        "layers_with_cache": len(per_layer_stats),
        "per_layer_stats": per_layer_stats,
    }


if __name__ == "__main__":
    print("="*60)
    print("Unsloth Spectral Integration Module")
    print("="*60)
    print("\nThis module provides monkey-patching for Unsloth models.")
    print("Usage:")
    print("  from unsloth_spectral import patch_unsloth_attention")
    print("  model = FastLanguageModel.from_pretrained(...)")
    print("  patch_unsloth_attention(model)")
    print("  # Now model.generate() uses spectral cache!")
    print("="*60)

